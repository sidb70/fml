{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import bertopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph Harold Greenberg (May 28, 1915 – May 7, 2001) was an American linguist, known mainly for his work concerning linguistic typology and the genetic classification of languages.\n",
      "\n",
      "Life\n",
      "\n",
      "Early life and education \n",
      "\n",
      "Joseph Greenberg was born on May 28, 1915 to Jewish parents in Brooklyn, New York. His first great interest was music. At the age of 14, he gave a piano concert in Steinway Hall. He continued to play the piano frequently throughout his life.\n",
      "\n",
      "After finishing high school, he decided to pursue a scholarly career rather than a musical one. He enrolled at Columbia University in New York. During his senior year, he attended a class taught by Franz Boas concerning American Indian languages. With references from Boas and Ruth Benedict, he was accepted as a graduate student by Melville J. Herskovits at Northwestern University in Chicago. During the course of his graduate studies, Greenberg did fieldwork among the Hausa people of Nigeria, where he learned the Hausa language. The subject of his doctoral dissertation was the influence of Islam on a Hausa group that, unlike most others, had not converted to it.\n",
      "\n",
      "During 1940, he began postdoctoral studies at Yale University. These were interrupted by service in the U.S. Army Signal Corps during World War II, for which he worked as a codebreaker and participated with the landing at Casablanca. Before leaving for Europe during 1943, Greenberg married Selma Berkowitz, whom he had met during his first year at Columbia University.\n",
      "\n",
      "Career\n",
      "After the war, Greenberg taught at the University of Minnesota before returning to Columbia University during 1948 as a teacher of anthropology. While in New York, he became acquainted with Roman Jakobson and André Martinet. They introduced him to the Prague school of structuralism, which influenced his work.\n",
      "\n",
      "During 1962, Greenberg relocated to the anthropology department of Stanford University in California, where he continued to work for the rest of his life. During 1965 Greenberg served as president of the African Studies Association. He received during 1996 the highest award for a scholar in Linguistics, the Gold Medal of Philology.\n",
      "\n",
      "Contributions to linguistics\n",
      "\n",
      "Linguistic typology \n",
      "\n",
      "Greenberg's reputation rests partly on his contributions to synchronic linguistics and the quest to identify linguistic universals. During the late 1950s, Greenberg began to examine languages covering a wide geographic and genetic distribution. He located a number of interesting potential universals as well as many strong cross-linguistic tendencies.\n",
      "\n",
      "In particular, Greenberg conceptualized the idea of \"implicational universal\", which has the form, \"if a language has structure X, then it must also have structure Y.\" For example, X might be \"mid front rounded vowels\" and Y \"high front rounded vowels\" (for terminology see phonetics). Many scholars adopted this kind of research following Greenberg's example and it remains important in synchronic linguistics.\n",
      "\n",
      "Like Noam Chomsky, Greenberg sought to discover the universal structures on which human language is based. Unlike Chomsky, Greenberg's method was functionalist, rather than formalist. An argument to reconcile the Greenbergian and Chomskyan methods can be found in Linguistic Universals (2006), edited by Ricardo Mairal and Juana Gil .\n",
      "\n",
      "Many who are strongly opposed to Greenberg's methods of language classification (see below) acknowledge the importance of his typological work. During 1963 he published an article that was extremely influential: \"Some universals of grammar with particular reference to the order of meaningful elements\".\n",
      "\n",
      "Mass comparison \n",
      "\n",
      "Greenberg rejected the opinion, prevalent among linguists since the mid-20th century, that comparative reconstruction was the only method to discover relationships between languages. He argued that genetic classification is methodologically prior to comparative reconstruction, or the first stage of it: one cannot engage in the comparative reconstruction of languages until one knows which languages to compare (1957:44).\n",
      "\n",
      "He also criticized the prevalent opinion that comprehensive comparisons of two languages at a time (which commonly take years to perform) could establish language families of any size. He argued that, even for 8 languages, there are already 4,140 ways to classify them into distinct families, while for 25 languages there are 4,749,027,089,305,918,018 ways (1957:44). For comparison, the Niger–Congo family is said to have some 1,500 languages. He thought language families of any size needed to be established by some scholastic means other than bilateral comparison. The theory of mass comparison is an attempt to demonstrate such means.\n",
      "\n",
      "Greenberg argued for the virtues of breadth over depth. He advocated restricting the amount of material to be compared (to basic vocabulary, morphology, and known paths of sound change) and increasing the number of languages to be compared to all the languages in a given area. This would make it possible to compare numerous languages reliably. At the same time, the process would provide a check on accidental resemblances through the sheer number of languages under review. The mathematical probability that resemblances are accidental decreases strongly with the number of languages concerned (1957:39).\n",
      "\n",
      "Greenberg used the premise that mass \"borrowing\" of basic vocabulary is unknown. He argued that borrowing, when it occurs, is concentrated in cultural vocabulary and clusters \"in certain semantic areas\", making it easy to detect (1957:39). With the goal of determining broad patterns of relationship, the idea was not to get every word right but to detect patterns. From the beginning with his theory of mass comparison, Greenberg addressed why chance resemblance and borrowing were not obstacles to its being useful. Despite that, critics consider those phenomena caused difficulties for his theory.\n",
      "\n",
      "Greenberg first termed his method \"mass comparison\" in an article of 1954 (reprinted in Greenberg 1955). As of 1987, he replaced the term \"mass comparison\" with \"multilateral comparison\", to emphasize its contrast with the bilateral comparisons recommended by linguistics textbooks. He believed that multilateral comparison was not in any way opposed to the comparative method, but is, on the contrary, its necessary first step (Greenberg, 1957:44). According to him, comparative reconstruction should have the status of an explanatory theory for facts already established by language classification (Greenberg, 1957:45).\n",
      "\n",
      "Most historical linguists (Campbell 2001:45) reject the use of mass comparison as a method for establishing genealogical relationships between languages. Among the most outspoken critics of mass comparison have been Lyle Campbell, Donald Ringe, William Poser, and the late R. Larry Trask.\n",
      "\n",
      "Genetic classification of languages\n",
      "\n",
      "Languages of Africa \n",
      "\n",
      "Greenberg is known widely for his development of a classification system for the languages of Africa, which he published as a series of articles in the Southwestern Journal of Anthropology from 1949 to 1954 (reprinted together as a book, The Languages of Africa, in 1955). He revised the book and published it again during 1963, followed by a nearly identical edition of 1966 (reprinted without change during 1970). A few more changes of the classification were made by Greenberg in an article during 1981.\n",
      "\n",
      "Greenberg grouped the hundreds of African languages into four families, which he dubbed Afroasiatic, Nilo-Saharan, Niger–Congo, and Khoisan. During the course of his work, Greenberg invented the term \"Afroasiatic\" to replace the earlier term \"Hamito-Semitic\", after showing that the Hamitic group, accepted widely since the 19th century, is not a valid language family. Another major feature of his work was to establish the classification of the Bantu languages, which occupy much of sub-Saharan Africa, as a part of the Niger–Congo family, rather than as an independent family as many Bantuists had maintained.\n",
      "\n",
      "Greenberg's classification rested largely in evaluating competing earlier classifications. For a time, his classification was considered bold and speculative, especially the proposal of a Nilo-Saharan language family. Now, apart from Khoisan, it is generally accepted by African specialists and has been used as a basis for further work by other scholars.\n",
      "\n",
      "Greenberg's work on African languages has been criticised by Lyle Campbell and Donald Ringe, who do not believe that his classification is justified by his data and request a re-examination of his macro-phyla by \"reliable methods\" (Ringe 1993:104). Harold Fleming and Lionel Bender, who were sympathetic to Greenberg's classification, acknowledged that at least some of his macrofamilies (particularly the Nilo-Saharan and the Khoisan macrofamiles) are not accepted completely by most linguists and may need to be divided (Campbell 1997). Their objection was methodological: if mass comparison is not a valid method, it cannot be expected to have brought order successfully out of the confusion of African languages.\n",
      "\n",
      "By contrast, some linguists have sought to combine Greenberg's four African families into larger units. In particular, Edgar Gregersen (1972) proposed joining Niger–Congo and Nilo-Saharan into a larger family, which he termed Kongo-Saharan. Roger Blench (1995) suggests Niger–Congo is a subfamily of Nilo-Saharan.\n",
      "\n",
      "The languages of New Guinea, Tasmania, and the Andaman Islands\n",
      "\n",
      "During 1971 Greenberg proposed the Indo-Pacific macrofamily, which groups together the Papuan languages (a large number of language families of New Guinea and nearby islands) with the native languages of the Andaman Islands and Tasmania but excludes the Australian Aboriginal languages. Its principal feature was to reduce the manifold language families of New Guinea to a single genetic unit. This excludes the Austronesian languages, which have been established as associated with a more recent migration of people.\n",
      "\n",
      "Greenberg's subgrouping of these languages has not been accepted by the few specialists who have worked on the classification of these languages. However, the work of Stephen Wurm (1982) and Malcolm Ross (2005) has provided considerable evidence for his once-radical idea that these languages form a single genetic unit. Wurm stated that the lexical similarities between Great Andamanese and the West Papuan and Timor–Alor families \"are quite striking and amount to virtual formal identity [...] in a number of instances.\" He believes this to be due to a linguistic substratum.\n",
      "\n",
      "The languages of the Americas\n",
      "\n",
      "Most linguists concerned with the native languages of the Americas classify them into 150 to 180 independent language families. Some believe that two language families, Eskimo–Aleut and Na-Dené, were distinct, perhaps the results of later migrations into the New World.\n",
      "\n",
      "Early on, Greenberg (1957:41, 1960) became convinced that many of the language groups considered unrelated could be classified into larger groupings. In his 1987 book Language in the Americas, while agreeing that the Eskimo–Aleut and Na-Dené groupings as distinct, he proposed that all the other Native American languages belong to a single language macro-family, which he termed Amerind.\n",
      "\n",
      "Language in the Americas has generated lively debate, but has been criticized strongly; it is rejected by most specialists of indigenous languages of the Americas and also by most historical linguists. Specialists of the individual language families have found extensive inaccuracies and errors in Greenberg's data, such as including data from non-existent languages, erroneous transcriptions of the forms compared, misinterpretations of the meanings of words used for comparison, and entirely spurious forms.\n",
      "\n",
      "Historical linguists also reject the validity of the method of multilateral (or mass) comparison upon which the classification is based. They argue that he has not provided a convincing case that the similarities presented as evidence are due to inheritance from an earlier common ancestor rather than being explained by a combination of errors, accidental similarity, excessive semantic latitude in comparisons, borrowings, onomatopoeia, etc.\n",
      "\n",
      "However, Harvard geneticist David Reich notes that recent genetic studies have identified patterns that support Greenberg's Amerind classification: the “First American” category. “The cluster of populations that he predicted to be most closely related based on language were in fact verified by the genetic patterns in populations for which data are available.” Nevertheless, this category of “First American” people also interbred with and contributed a significant amount of genes to the ancestors of both Eskimo-Aleut and Na-Dené populations, with 60% and 90% \"First American\" DNA respectively constituting the genetic makeup of the two groups.\n",
      "\n",
      "The languages of northern Eurasia \n",
      "\n",
      "Later in his life, Greenberg proposed that nearly all of the language families of northern Eurasia belong to a single higher-order family, which he termed Eurasiatic. The only exception was Yeniseian, which has been related to a wider Dené–Caucasian grouping, also including Sino-Tibetan.  During 2008 Edward Vajda related Yeniseian to the Na-Dené languages of North America as a Dené–Yeniseian family.\n",
      "\n",
      "The Eurasiatic grouping resembles the older Nostratic groupings of Holger Pedersen and Vladislav Illich-Svitych by including Indo-European, Uralic, and Altaic. It differs by including Nivkh, Japonic, Korean, and Ainu (which the Nostraticists had excluded from comparison because they are single languages rather than language families) and in excluding Afroasiatic. At about this time, Russian Nostraticists, notably Sergei Starostin, constructed a revised version of Nostratic. It was slightly larger than Greenberg's grouping but it also excluded Afroasiatic.\n",
      "\n",
      "Recently, a consensus has been emerging among proponents of the Nostratic hypothesis. Greenberg basically agreed with the Nostratic concept, though he stressed a deep internal division between its northern 'tier' (his Eurasiatic) and a southern 'tier' (principally Afroasiatic and Dravidian).\n",
      "\n",
      "The American Nostraticist Allan Bomhard considers Eurasiatic a branch of Nostratic, alongside other branches: Afroasiatic, Elamo-Dravidian, and Kartvelian. Similarly, Georgiy Starostin (2002) arrives at a tripartite overall grouping: he considers Afroasiatic, Nostratic and Elamite to be roughly equidistant and more closely related to each other than to any other language family. Sergei Starostin's school has now included Afroasiatic in a broadly defined Nostratic. They reserve the term Eurasiatic to designate the narrower subgrouping, which comprises the rest of the macrofamily. Recent proposals thus differ mainly on the precise inclusion of Dravidian and Kartvelian.\n",
      "\n",
      "Greenberg continued to work on this project after he was diagnosed with incurable pancreatic cancer and until he died during May 2001. His colleague and former student Merritt Ruhlen ensured the publication of the final volume of his Eurasiatic work (2002) after his death.\n",
      "\n",
      "Selected works by Joseph H. Greenberg\n",
      "\n",
      "Books \n",
      "  (Photo-offset reprint of the SJA articles with minor corrections.)\n",
      " \n",
      "  (Heavily revised version of Greenberg 1955. From the same publisher: second, revised edition, 1966; third edition, 1970. All three editions simultaneously published at The Hague by Mouton & Co.)\n",
      "  (Reprinted 1980 and, with a foreword by Martin Haspelmath, 2005.)\n",
      "\n",
      "Books (editor) \n",
      "  (Second edition 1966.)\n",
      "\n",
      "Articles, reviews, etc. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  (Reprinted in Genetic Linguistics, 2005.)\n",
      " \n",
      "  (In second edition of Universals of Language, 1966: pp. 73–113.)\n",
      " \n",
      " \n",
      "  (Reprinted in Genetic Linguistics, 2005.)\n",
      "\n",
      "Bibliography\n",
      "\n",
      "Blench, Roger. 1995. \"Is Niger–Congo simply a branch of Nilo-Saharan?\" In Fifth Nilo-Saharan Linguistics Colloquium, Nice, 24–29 August 1992: Proceedings, edited by Robert Nicolaï and Franz Rottland. Cologne: Köppe Verlag, pp. 36–49.\n",
      "\n",
      "Campbell, Lyle. 1997. American Indian Languages: The Historical Linguistics of Native America. New York: Oxford University Press. .\n",
      "Campbell, Lyle. 2001. \"Beyond the comparative method.\" In Historical Linguistics 2001: Selected Papers from the 15th International Conference on Historical Linguistics, Melbourne, 13–17 August 2001, edited by Barry J. Blake, Kate Burridge, and Jo Taylor.\n",
      "Diamond, Jared. 1997. Guns, Germs and Steel: The Fates of Human Societies. New York: Norton. .\n",
      "\n",
      "Mairal, Ricardo and Juana Gil. 2006. Linguistic Universals. Cambridge–NY: Cambridge University Press. .\n",
      "\n",
      "Ross, Malcolm. 2005. \"Pronouns as a preliminary diagnostic for grouping Papuan languages.\" In Papuan Pasts: Cultural, Linguistic and Biological Histories of Papuan-speaking Peoples, edited by Andrew Pawley, Robert Attenborough, Robin Hide, and Jack Golson. Canberra: Pacific Linguistics, pp. 15–66.\n",
      "Wurm, Stephen A. 1982. The Papuan Languages of Oceania. Tübingen: Gunter Narr.\n",
      "\n",
      "See also\n",
      "\n",
      "Linguistic universal\n",
      "Monogenesis (linguistics)\n",
      "Nostratic languages\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "Joseph Greenberg at work; a portrait of himself\n",
      "\"What we all spoke when the world was young\" by Nicholas Wade, New York Times (February 1, 2000)\n",
      "Memorial Resolution\n",
      "\"Complete bibliography of the publications of Joseph H. Greenberg\" by William Croft (2003)\n",
      "\n",
      "Category:1915 births\n",
      "Category:2001 deaths\n",
      "Category:20th-century linguists\n",
      "Category:20th-century anthropologists\n",
      "Category:20th-century non-fiction writers\n",
      "Category:Anthropological linguists\n",
      "Category:American social scientists\n",
      "Category:Linguists from the United States\n",
      "Category:American Africanists\n",
      "Category:Jewish American military personnel\n",
      "Category:Paleolinguists\n",
      "Category:Columbia University faculty\n",
      "Category:Stanford University Department of Anthropology faculty\n",
      "Category:Fellows of the American Academy of Arts and Sciences\n",
      "Category:Members of the United States National Academy of Sciences\n",
      "Category:United States Army personnel\n",
      "Category:American army personnel of World War II\n",
      "Category:Guggenheim Fellows\n",
      "Category:American expatriates in Nigeria\n",
      "Category:People from Brooklyn\n",
      "Category:Linguists of Eskimo–Aleut languages\n",
      "Category:Linguists of Hokan languages\n",
      "Category:Jewish scientists\n",
      "Category:Linguists of Papuan languages\n",
      "Category:Linguists of Amerind languages\n",
      "Category:Linguists of Andamanese languages\n",
      "Category:Linguists of Tasmanian languages\n",
      "Category:Linguists of Niger–Congo languages\n",
      "Category:Linguists of Afroasiatic languages\n",
      "Category:Linguists of Eurasiatic languages\n",
      "Category:Linguistic Society of America presidents\n",
      "Category:Columbia College (New York) alumni\n",
      "Category:Linguists of indigenous languages of the Americas\n",
      "Pauline Donalda,  (March 5, 1882 – October 22, 1970) was a Canadian operatic soprano.\n",
      "\n",
      "Early life and education\n",
      "Donalda was born Pauline Lightstone in Montreal, Quebec, the daughter of Jewish parents who changed their surname from Lichtenstein to Lightstone after immigrating from Russia and Poland. She studied with Clara Lichtenstein (no relation) at Royal Victoria College, part of McGill University. In 1902, went to the Conservatoire de Paris on a grant from Donald Smith, Lord Strathcona, the patron of RVC.  There, she studied voice with Edmond Duvernoy.  She adopted the stage name Donalda in honour of her patron.\n",
      "\n",
      "Career\n",
      "With the help of composer Jules Massenet, Donalda made her debut in 1904 in Nice, singing the title role in his opera Manon.  The following year, she debuted in London, singing the role of Micaëla in Carmen. Donalda was the first to sing the roles of Concepción in Maurice Ravel's L'heure espagnole and Ah-joe in Franco Leoni's L'Oracolo at Covent Garden. In November 1906, she returned to Montreal to sing in a recital at the Montreal Arena with her new husband, baritone Paul Seveilhac. The following month, she began a season with Oscar Hammerstein's new Manhattan Opera House. She returned to Europe in 1907, singing principally in London and Paris.\n",
      "\n",
      "Donalda was in Canada when World War I broke out.  She chose to remain in the country, singing in concerts and music halls, with occasional appearances in New York and Boston. In Montreal, she organized the Donalda Sunday Afternoon Concerts, donating the proceeds to war charities.  She returned to Paris in 1917, and married her second husband, Mischa Léon, there the following year.\n",
      "\n",
      "In 1922, Donalda opened a teaching studio in Paris where she taught many students over the next few years. She moved back to Montreal in 1937 and opened a studio there. Her students in Montreal included Robert Savoie. She founded the Opera Guild of Montreal in 1942, serving as president and artistic director until 1969.\n",
      "\n",
      "In 1967, she was made an Officer of the Order of Canada \"for her contribution to the arts, especially opera, as a singer and founder of the Opera Guild in Montreal.\"\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " The Virtual Gramophone's Pauline Lightstone Donalda biography\n",
      " portrait\n",
      "\n",
      "Category:1882 births\n",
      "Category:1970 deaths\n",
      "Category:Canadian female singers\n",
      "Category:Canadian operatic sopranos\n",
      "Category:Jewish Canadian musicians\n",
      "Category:Musicians from Montreal\n",
      "Category:Officers of the Order of Canada\n",
      "Category:20th-century opera singers\n",
      "Category:20th-century women singers\n",
      "This is a list of German football transfers in the summer transfer window 2017 by club. Only transfers of the Bundesliga, and 2. Bundesliga are included.\n",
      "\n",
      "Bundesliga\n",
      "\n",
      "Note: Flags indicate national team as has been defined under FIFA eligibility rules. Players may hold more than one non-FIFA nationality.\n",
      "\n",
      "FC Bayern Munich\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "RB Leipzig\n",
      "\n",
      "In:                                       \n",
      "\n",
      "Out:\n",
      "\n",
      "Borussia Dortmund\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1899 Hoffenheim\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FC Köln\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Hertha BSC\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "SC Freiburg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Werder Bremen\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Borussia Mönchengladbach\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "FC Schalke 04\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Eintracht Frankfurt\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Bayer 04 Leverkusen\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "FC Augsburg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Hamburger SV\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FSV Mainz 05\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "VfL Wolfsburg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "VfB Stuttgart\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Hannover 96\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "2. Bundesliga\n",
      "\n",
      "FC Ingolstadt 04\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "SV Darmstadt 98\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Eintracht Braunschweig\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FC Union Berlin\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Dynamo Dresden\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FC Heidenheim\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "FC St. Pauli\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "SpVgg Greuther Fürth\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "VfL Bochum\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "SV Sandhausen\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Fortuna Düsseldorf\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FC Nürnberg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "1. FC Kaiserslautern\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Erzgebirge Aue\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Arminia Bielefeld\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "MSV Duisburg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Holstein Kiel\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "Jahn Regensburg\n",
      "\n",
      "In:\n",
      "\n",
      "Out:\n",
      "\n",
      "See also\n",
      " 2017–18 Bundesliga\n",
      " 2017–18 2. Bundesliga\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      " Official site of the DFB \n",
      " Kicker.de \n",
      " Official site of the Bundesliga \n",
      " Official site of the Bundesliga\n",
      "\n",
      "Category:Football transfers summer 2017\n",
      "Trans\n",
      "2017\n",
      "Lester Hudson III (born August 7, 1984) is an American professional basketball player for the Liaoning Flying Leopards of the Chinese Basketball Association (CBA). In the 2007–08 season, Hudson recorded the only quadruple-double in NCAA Division I men's basketball history. At the conclusion of the season, he declared himself for the 2008 NBA draft, but later withdrew. Hudson was drafted by the Boston Celtics with the 58th pick of the 2009 NBA draft, but was later waived. He was then signed by the Memphis Grizzlies. Hudson later played for the Washington Wizards and Cleveland Cavaliers before returning to the Grizzlies in April 2012.\n",
      "\n",
      "College career\n",
      "Hudson put together one of the best all-around seasons in Ohio Valley Conference history in 2007–08 and helped UT Martin make a six-game improvement in its league record from the previous year. The Skyhawks were picked last (11th) in a preseason poll but finished fourth and qualified for the OVC Tournament. \n",
      "\n",
      "Hudson made school, conference and NCAA history during the season, including becoming the first Division I men's player to record a quadruple-double (25 points, 12 rebounds, 10 assists and 10 steals vs. Central Baptist College) in a college game on November 13, 2007. (Counting Ann Meyers' 1978 quadruple-double for UCLA, at a time when women's college sports were governed by the AIAW instead of the NCAA, five women have recorded quadruple-doubles in NCAA Division I women's play, the most recent being Shakyla Hill of Grambling State on February 2nd, 2018.\n",
      "\n",
      "He also had a triple-double, eight additional double-doubles and cracked the 30-point plateau 11 times during his rookie season in the league. He finished the season ranked fifth nationally in points per game (25.7); fourth in steals (2.8) and tenth in 3-pointers made per game (3.8). He also ranked among the top 90 nationally in each rebounding (79th, 7.8/game), assists (88th, 4.5/game), 3-point percentage (72nd, 38.8%) and free throw percentage (67th, 83.4%). \n",
      "\n",
      "A Memphis, Tennessee native who transferred from Southwest Tennessee Community College, Hudson is the first UT Martin player to receive the OVC Player of the Year award. He was again honored in the 2008–09 season, the first back to back win since Murray State University's Marcus Brown won in the 1994-1995 and 1995-1996 seasons.  He was also named to the OVC All-Newcomer squad, became just the fourth Skyhawk ever named to the All-OVC first-team (2007–2008 and 2008–2009,) and was named UT Martin's Bob Carroll Male Athlete of the Year. \n",
      "\n",
      "With All-American honors from the Associated Press and Collegeinsider.com, Hudson became the first player in UT Martin history to claim the honors since the school started playing in Division I.\n",
      "\n",
      "Professional career\n",
      "Over the first few months of the 2009–10 season, Hudson split time between the Celtics and their NBADL affiliate, the Maine Red Claws. On January 2, an injury ravaged Celtics team turned to Hudson during a game against the Toronto Raptors. On January 6, 2010, Hudson was waived by the Celtics, but he was claimed by the Memphis Grizzlies two days later. The Grizzlies assigned him to the Dakota Wizards of the D-League on February 28, 2010.\n",
      "\n",
      "Hudson was waived on July 1, 2010 by the Grizzlies.\n",
      "\n",
      "He played for the Washington Wizards during the NBA's 2010 summer league.\n",
      "\n",
      "Hudson was invited to Wizards' training camp for the 2010–11 NBA season and made the team. After playing six games for them, he was waived on November 23, 2010. He was re-signed on December 20, 2010, but waived again on January 5, 2011.\n",
      "\n",
      "In January 2011 he signed with the Guangdong Southern Tigers in China. He joined Qingdao DoubleStar in November 2011.\n",
      "\n",
      "On March 30, 2012, Hudson was signed by the Cleveland Cavaliers to the first of two \n",
      "10-day contracts. He quickly established himself, scoring at least 23 points in three consecutive games from April 6 to April 10. He also hit two last-second shots to force two games into overtime. On April 20, 2012, after his second 10-day contract expired, he joined the Memphis Grizzlies for the remainder of the 2011–12 season.\n",
      "\n",
      "On October 12, 2012, he signed with the Dongguan Leopards of China. On March 15, 2013, he was acquired by the Austin Toros of the NBA D-League.\n",
      "\n",
      "On October 1, 2013, he signed with the Utah Jazz. He was later waived by the Jazz on October 26. In November 2013, he signed with Xinjiang Flying Tigers. \n",
      "\n",
      "On October 14, 2014, he signed with the Liaoning Flying Leopards for the 2014–15 CBA season.\n",
      "\n",
      "On March 29, 2015, Hudson signed a 10-day contract with the Los Angeles Clippers. On April 11, he signed a multi-year deal with the Clippers. On July 15, he was waived by the Clippers.\n",
      "\n",
      "On September 13, 2015, Hudson returned to the Liaoning Flying Leopards, signing a new three-year deal with the club. He re-signed with the team on August 10, 2018.\n",
      "\n",
      "NBA career statistics\n",
      "\n",
      "Regular season\n",
      "\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | Boston\n",
      "| 16 || 0 || 4.4 || .389 || .600 || .833 || .5 || .5 || .2 || .1 || 1.4\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | Memphis\n",
      "| 9 || 0 || 6.8 || .400 || .182 || .857 || 1.1 || .6 || .6 || .1 || 4.0\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | Washington\n",
      "| 11 || 0 || 6.6 || .250 || .267 || .500 || .5 || 1.5 || .4 || .1 || 1.6\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | Cleveland\n",
      "| 13 || 0 || 24.2 || .391 || .246 || .842 || 3.5 || 2.7 || 1.1 || .2 || 12.7\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | Memphis\n",
      "| 3 || 0 || 6.7 || .273 || .333 || .667 || .0 || .3 || .0 || .0 || 3.0\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | L.A. Clippers\n",
      "| 5|| 0 || 11.2 || .429|| .500|| .750 || 1.6 || 1.0 || 1.2 || .2 || 3.6\n",
      "|- class=\"sortbottom\"\n",
      "| style=\"text-align:center;\" colspan=\"2\"| Career\n",
      "| 52 || 0 || 10.3 || .375 || .277 || .806 || 1.4 || 1.2|| .6 || .1 || 4.7\n",
      "\n",
      "Playoffs\n",
      "\n",
      "|-\n",
      "| align=\"left\" | \n",
      "| align=\"left\" | L.A. Clippers\n",
      "| 7 || 0 || 5.4 || .429 || .286 || - || .1 || 1.0 || .3 || .1 ||2.0\n",
      "\n",
      "See also\n",
      " 2009 NCAA Men's Basketball All-Americans\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "UTMSports.com player piece\n",
      "ESPN.com profile\n",
      "\n",
      "Category:1984 births\n",
      "Category:Living people\n",
      "Category:African-American basketball players\n",
      "Category:American expatriate basketball people in China\n",
      "Category:American men's basketball players\n",
      "Category:Austin Toros players\n",
      "Category:Basketball players from Tennessee\n",
      "Category:Boston Celtics draft picks\n",
      "Category:Boston Celtics players\n",
      "Category:Cleveland Cavaliers players\n",
      "Category:Dakota Wizards players\n",
      "Category:Guangdong Southern Tigers players\n",
      "Category:Junior college men's basketball players in the United States\n",
      "Category:Liaoning Flying Leopards players\n",
      "Category:Los Angeles Clippers players\n",
      "Category:Maine Red Claws players\n",
      "Category:Memphis Grizzlies players\n",
      "Category:Point guards\n",
      "Category:Qingdao DoubleStar players\n",
      "Category:Shenzhen Aviators players\n",
      "Category:Shooting guards\n",
      "Category:Sportspeople from Memphis, Tennessee\n",
      "Category:UT Martin Skyhawks men's basketball players\n",
      "Category:Washington Wizards players\n",
      "Category:Xinjiang Flying Tigers players\n",
      "Monique Ganderton (born August 6, 1980) is a Canadian stunt woman and actress who works in television and film.\n",
      "\n",
      "Ganderton was born in Edmonton, Alberta. She started out in modeling before moving to stunt work. Standing 5 ft 11 in (1.8 m) tall, she has doubled for tall actresses like Tricia Helfer, Rachel Nichols,  Leelee Sobieski, Bridget Moynahan, Daryl Hannah, Rebecca Romijn and Famke Janssen. In 2009, she was cast as Alia, a recurring role in Season 9 of Smallville. In the 2019 film Avengers: Endgame, Ganderton was both a stunt woman and a stunt coordinator.\n",
      "\n",
      "Filmography\n",
      "\n",
      "Stunt work on TV\n",
      "Masters of Horror (2002)\n",
      "Battlestar Galactica (2003-2004)\n",
      "Supernatural (2005)\n",
      "The 4400 (2007)\n",
      "Flash Gordon (2007–2008)\n",
      "Blood Ties (2007)\n",
      "Samurai Girl (2008)\n",
      "Smallville (2008)\n",
      "Stargate Universe (2009)\n",
      "Human Target (2010)\n",
      "Continuum (2013)\n",
      "\n",
      "Stunt work on film\n",
      "The Recruit (2003)\n",
      "White Chicks (2004)\n",
      "I, Robot (2004)\n",
      "Resident Evil: Apocalypse (2004)\n",
      "Fantastic Four (2005)\n",
      "Underworld: Evolution (2006)\n",
      "Final Days of Planet Earth (2006)\n",
      "Engaged to Kill (2006)\n",
      "X-Men: The Last Stand (2006)\n",
      "A Girl Like Me: The Gwen Araujo Story (2006)\n",
      "12 Hours to Live (2006)\n",
      "The Wicker Man (2006)\n",
      "Underfunded (2006)\n",
      "In the Name of the King: A Dungeon Siege Tale (2007)\n",
      "White Noise: The Light (2007)\n",
      "Butterfly on a Wheel (2007)\n",
      "When a Man Falls in the Forest (2007)\n",
      "88 Minutes (2007)\n",
      "Fantastic Four: Rise of the Silver Surfer (2007)\n",
      "Alien Agent (2007)\n",
      "Postal (2007)\n",
      "Walk All Over Me (2007)\n",
      "Devil's Diary (2007)\n",
      "Battlestar Galactica: Razor (2007)\n",
      "Another Cinderella Story (2008)\n",
      "Joy Ride 2: Dead Ahead (2008)\n",
      "Watchmen (2009)\n",
      "The Imaginarium of Doctor Parnassus (2009)\n",
      "2012 (2009)\n",
      "Tron: Legacy (2010)\n",
      "Dancing Ninja (2010)\n",
      "The Cabin in the Woods (2011)\n",
      "Conan the Barbarian (2011)\n",
      "American Ultra (2015)\n",
      "Suicide Squad (2016)\n",
      "Avengers: Infinity War (2018)\n",
      "Avengers: Endgame (2019) - additionally the stunt coordinator\n",
      "\n",
      "Actress on TV\n",
      "Hendrix (2000)\n",
      "Leap Years (2001)\n",
      "Mutant X (2001)\n",
      "1-800-Missing (2004)\n",
      "The L Word (2005)\n",
      "The 4400 (2006)\n",
      "Blood Ties (2007)\n",
      "Fallen (2007)\n",
      "Supernatural (2007)\n",
      "Smallville (2009–2010)\n",
      "Continuum (TV series) (2012) as Yvonne Ducelle\n",
      "Sleepy Hollow (2013) as Serilda\n",
      "The Tomorrow People (2014) as Nelly\n",
      "The 100 (2014) as Aurora Blake\n",
      "Unreal (2016) as Brandi\n",
      "\n",
      "Actress on Film\n",
      "Chicago (2002)\n",
      "The Wicker Man (2006)\n",
      "The Secrets of Comfort House (2006)\n",
      "A.M.P.E.D. (2007)\n",
      "When a Man Falls in the Forest (2007)\n",
      "Numb (2007)\n",
      "Inseparable (2008)\n",
      "Another Day in Hell (2009)\n",
      "Merlin and the Book of Beasts (2009)\n",
      "The King of Fighters (2010)\n",
      "Icarus (2009)\n",
      "Bad Meat (2009)\n",
      "30 Days of Night: Dark Days (2010)\n",
      "The Twilight Saga: Eclipse (2010)\n",
      "Unrivaled (2010)\n",
      "Hard Ride to Hell (2010)\n",
      "The Cabin in the Woods (2011)\n",
      "Hansel & Gretel: Witch Hunters (2013)\n",
      "The Package (2013)\n",
      "American Ultra (2015)\n",
      "X-Men: Apocalypse (2016)\n",
      "S.W.A.T.: Under Siege (2017)\n",
      "\n",
      "References\n",
      "\n",
      "External links \n",
      "\n",
      "Category:1980 births\n",
      "Category:Actresses from Edmonton\n",
      "Category:Canadian film actresses\n",
      "Category:Canadian stunt performers\n",
      "Category:Canadian television actresses\n",
      "Category:Living people\n",
      "Category:21st-century Canadian actresses\n",
      "This is a list of women artists who were born in Latvia or whose artworks are closely associated with that country.\n",
      "\n",
      "B\n",
      "Aleksandra Belcova (1892–1981), painter\n",
      "Biruta Baumane (born 1922), painter\n",
      "\n",
      "D\n",
      "Lilija Dinere (born 1955), painter, illustrator\n",
      "\n",
      "K\n",
      "Ingrīda Kadaka (born 1967), book designer, illustrator\n",
      "Aina Karlsone (1935–2012), artist, writer\n",
      "\n",
      "O\n",
      "Simona Orinska (born 1978), contemporary artist\n",
      "\n",
      "P\n",
      "Tatyana Palchuk (born 1954), painter\n",
      "Lucia Peka (1912–1991), Latvian-American painter\n",
      "Līga Purmale (born 1948), painter\n",
      "\n",
      "S\n",
      "Daina Skadmane (1990–2013), painter, lithographer\n",
      "Roze Stiebra (born 1942), animator\n",
      "\n",
      "-\n",
      "Latvian\n",
      "Artists\n",
      "Artists, women\n",
      "The white bikini of Ursula Andress (also known as the Dr. No bikini) was a white bikini worn by Ursula Andress as Honey Ryder in the 1962 James Bond film, Dr. No. It is cited as the most famous bikini of all time and an iconic moment in cinematic and fashion history.\n",
      "\n",
      "Andress's white bikini is regarded as monumental in the history of the bikini, and sales of the two-piece bikini rocketed after the appearance of Andress in Dr. No. The lower part of the bikini features a wide white British Army belt with brass buckles and fittings, and a scabbard on the left side to hold a large knife.\n",
      "\n",
      "History \n",
      "\n",
      "The first bikini had been worn at a Paris fashion show in 1946, but in the 1950s the bikini was still seen as something of a taboo. Andress' bikini arrived at a key moment in the history of women's fashion, coming at the \"birth of the sexual revolution\": the 1960s.\n",
      "\n",
      "In the corresponding scene of the original novel, the character Honeychile Rider wears only a leather belt with a scabbard, and no bikini.\n",
      "\n",
      "Design \n",
      "\n",
      "Andress designed the bikini along with Dr. Nos costume designer Tessa Prendergast, whom she first met while living in Rome. Andress reported that when she arrived in Jamaica for filming, no costumes were ready. She worked with director Terence Young and the costume designer to create something that fit her 5'6\", 36–24–36 frame. It was made from ivory cotton and was the only one made and worn by her. It is a white belted bikini.\n",
      "\n",
      "Reception \n",
      "\n",
      "The Dr. No bikini is cited as the best known bikini of all time and an iconic moment in cinematic and fashion history. The moment in which Andress emerges from the sea in the white bikini has been cited amongst the greatest moments in film and one of its most erotic; in a 2003 UK Survey by Channel 4, it was voted number one in \"the 100 Greatest Sexy Moments\" of cinema. The scene has been widely emulated and parodied on screen since. The white bikini is regarded as perhaps the most important in the history of the bikini and sales of the two-piece bikini rocketed after the appearance of Andress in Dr. No. In a survey of 1000 women to celebrate the 60th anniversary of the bikini, Ursula Andress in her white bikini was voted \"The Ultimate Bikini Goddess\". Andress said that she owed her career to that white bikini, remarking, \"This bikini made me into a success. As a result of starring in Dr. No as the first Bond girl, I was given the freedom to take my pick of future roles and to become financially independent.\" Andress auctioned the bikini through Christie's auction house in London, selling it for £35,000 in 2001, less than the £40,000 it had been expected to sell for.\n",
      "\n",
      "Homage \n",
      "\n",
      "The bikini and scene in Dr. No of Andress emerging from the water was emulated by Heather Graham in a scene from Austin Powers: The Spy Who Shagged Me. It has also been emulated by Halle Berry, who wore an orange bikini with a toolbelt in the 2002 James Bond film Die Another Day.\n",
      "\n",
      "When Daniel Craig took over the role of James Bond in the 2006 film Casino Royale, he appeared in a similar scene, emerging from the ocean wearing only a pale blue pair of swim trunks. This thirteen-second shot, focused on Bond's body rather than that of a Bond Girl, was widely interpreted as a callback to Andress in Dr. No and featured heavily in the film's promotion, although Craig claims the resemblance did not occur to him until it was filmed.\n",
      "\n",
      "See also \n",
      " History of the bikini\n",
      " Bikini in popular culture\n",
      "\n",
      "References \n",
      "\n",
      "Category:1960s fashion\n",
      "Category:1962 clothing\n",
      "Category:Dr. No (film)\n",
      "Category:Individual bikinis\n",
      "Cellana radians, common name the radiate limpet, is a species of true limpet, a marine gastropod mollusc in the family Nacellidae, which is one of the true limpet families.\n",
      "\n",
      "Description\n",
      "The foot and the head are lightly colored whereas Cellana flava has darker colored soft tissues. The shell exhibits different morphological varieties, even in the same location. The shell can be grayish white on the outside with rather flat ribs that are somewhat darker. Its interior is iridescent and white, with gray muscle impressions. The apex is off-center and sometimes worn off.\n",
      "\n",
      "Ecology\n",
      "Cellana radians is found on rocks and other hard substrates in the littoral and sublittoral zones of the seas around New Zealand.\n",
      "\n",
      "References\n",
      "\n",
      " Nakano T. & Ozawa T. (2007). Worldwide phylogeography of limpets of the order Patellogastropoda: molecular, morphological and paleontological evidence. Journal of Molluscan Studies 73(1): 79–99\n",
      " Bruce A. Marshall, Molluscan and brachiopod taxa introduced by F. W. Hutton in The New Zealand journal of science; Journal of the Royal Society of New Zealand, Volume 25, Issue 4, 1995\n",
      "\n",
      "Category:Nacellidae\n",
      "Category:Gastropods of New Zealand\n",
      "Category:Gastropods described in 1791\n",
      "Onthophagus adelaidae is a species of beetle discovered by Frederick William Hope in 1846. No sub-species are listed at Catalogue of Life.\n",
      "\n",
      "References\n",
      "\n",
      "Category:Scarabaeinae\n",
      "Neal Alexander Scott Mackey (born 10 February 1983) is an English cricketer.  Mackey is a right-handed batsman who bowls right-arm medium-fast.  He was born in Leicester, Leicestershire.\n",
      "\n",
      "Mackey represented the Leicestershire Cricket Board in a single List A match against the Kent Cricket Board in the 2nd round of the 2003 Cheltenham & Gloucester Trophy which was held in 2002.  In his only List A match he scored 25 runs.\n",
      "\n",
      "In currently plays club cricket for Market Harborough Cricket Club in the Leicestershire Premier Cricket League.\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "Neal Mackey at Cricinfo\n",
      "Neal Mackey at CricketArchive\n",
      "\n",
      "Category:1983 births\n",
      "Category:Living people\n",
      "Category:Sportspeople from Leicester\n",
      "Category:English cricketers\n",
      "Category:Leicestershire Cricket Board cricketers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-08 17:07:36.581667: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-02-08 17:07:36.770427: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ds = tfds.load('wikipedia/20200301.en',split='train')\n",
    "for example in ds.take(10):\n",
    "    print(example['text'].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = [i['text'].numpy().decode('utf-8') for i in ds.take(5000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(bm25_weighting=False, reduce_frequent_words=False)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with \n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1,\n",
       "  22,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  37,\n",
       "  23,\n",
       "  12,\n",
       "  16,\n",
       "  -1,\n",
       "  0,\n",
       "  52,\n",
       "  50,\n",
       "  0,\n",
       "  44,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  19,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  16,\n",
       "  6,\n",
       "  4,\n",
       "  31,\n",
       "  34,\n",
       "  -1,\n",
       "  8,\n",
       "  20,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  43,\n",
       "  -1,\n",
       "  -1,\n",
       "  36,\n",
       "  3,\n",
       "  12,\n",
       "  -1,\n",
       "  12,\n",
       "  10,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  44,\n",
       "  3,\n",
       "  4,\n",
       "  -1,\n",
       "  31,\n",
       "  32,\n",
       "  -1,\n",
       "  8,\n",
       "  38,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  3,\n",
       "  1,\n",
       "  47,\n",
       "  21,\n",
       "  7,\n",
       "  5,\n",
       "  38,\n",
       "  24,\n",
       "  4,\n",
       "  29,\n",
       "  24,\n",
       "  21,\n",
       "  42,\n",
       "  58,\n",
       "  43,\n",
       "  15,\n",
       "  25,\n",
       "  10,\n",
       "  0,\n",
       "  15,\n",
       "  9,\n",
       "  0,\n",
       "  36,\n",
       "  11,\n",
       "  29,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  14,\n",
       "  9,\n",
       "  28,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  13,\n",
       "  -1,\n",
       "  47,\n",
       "  23,\n",
       "  2,\n",
       "  22,\n",
       "  37,\n",
       "  22,\n",
       "  -1,\n",
       "  0,\n",
       "  32,\n",
       "  12,\n",
       "  3,\n",
       "  41,\n",
       "  10,\n",
       "  -1,\n",
       "  20,\n",
       "  23,\n",
       "  -1,\n",
       "  38,\n",
       "  -1,\n",
       "  4,\n",
       "  2,\n",
       "  16,\n",
       "  28,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  40,\n",
       "  54,\n",
       "  4,\n",
       "  11,\n",
       "  18,\n",
       "  -1,\n",
       "  0,\n",
       "  36,\n",
       "  11,\n",
       "  15,\n",
       "  -1,\n",
       "  8,\n",
       "  40,\n",
       "  -1,\n",
       "  47,\n",
       "  40,\n",
       "  18,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  0,\n",
       "  50,\n",
       "  34,\n",
       "  9,\n",
       "  20,\n",
       "  9,\n",
       "  8,\n",
       "  51,\n",
       "  18,\n",
       "  -1,\n",
       "  39,\n",
       "  42,\n",
       "  29,\n",
       "  47,\n",
       "  3,\n",
       "  8,\n",
       "  -1,\n",
       "  26,\n",
       "  35,\n",
       "  -1,\n",
       "  56,\n",
       "  -1,\n",
       "  12,\n",
       "  35,\n",
       "  -1,\n",
       "  20,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  31,\n",
       "  42,\n",
       "  34,\n",
       "  21,\n",
       "  8,\n",
       "  4,\n",
       "  43,\n",
       "  -1,\n",
       "  0,\n",
       "  50,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  13,\n",
       "  1,\n",
       "  28,\n",
       "  30,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  6,\n",
       "  11,\n",
       "  34,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  36,\n",
       "  8,\n",
       "  51,\n",
       "  2,\n",
       "  13,\n",
       "  25,\n",
       "  16,\n",
       "  2,\n",
       "  0,\n",
       "  4,\n",
       "  39,\n",
       "  10,\n",
       "  8,\n",
       "  30,\n",
       "  46,\n",
       "  10,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  32,\n",
       "  43,\n",
       "  16,\n",
       "  11,\n",
       "  27,\n",
       "  22,\n",
       "  3,\n",
       "  -1,\n",
       "  14,\n",
       "  13,\n",
       "  8,\n",
       "  1,\n",
       "  -1,\n",
       "  39,\n",
       "  14,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  55,\n",
       "  2,\n",
       "  1,\n",
       "  24,\n",
       "  56,\n",
       "  -1,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  27,\n",
       "  6,\n",
       "  18,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  22,\n",
       "  36,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  14,\n",
       "  46,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  14,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  14,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  14,\n",
       "  28,\n",
       "  26,\n",
       "  41,\n",
       "  17,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  1,\n",
       "  -1,\n",
       "  15,\n",
       "  0,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  4,\n",
       "  -1,\n",
       "  12,\n",
       "  1,\n",
       "  6,\n",
       "  39,\n",
       "  30,\n",
       "  10,\n",
       "  18,\n",
       "  16,\n",
       "  42,\n",
       "  12,\n",
       "  8,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  52,\n",
       "  0,\n",
       "  12,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  -1,\n",
       "  2,\n",
       "  24,\n",
       "  13,\n",
       "  28,\n",
       "  1,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  35,\n",
       "  -1,\n",
       "  2,\n",
       "  8,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  20,\n",
       "  -1,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  15,\n",
       "  26,\n",
       "  1,\n",
       "  33,\n",
       "  40,\n",
       "  3,\n",
       "  22,\n",
       "  0,\n",
       "  21,\n",
       "  -1,\n",
       "  -1,\n",
       "  13,\n",
       "  -1,\n",
       "  28,\n",
       "  29,\n",
       "  14,\n",
       "  35,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  23,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  2,\n",
       "  -1,\n",
       "  24,\n",
       "  58,\n",
       "  8,\n",
       "  2,\n",
       "  -1,\n",
       "  2,\n",
       "  6,\n",
       "  17,\n",
       "  -1,\n",
       "  7,\n",
       "  15,\n",
       "  -1,\n",
       "  42,\n",
       "  4,\n",
       "  39,\n",
       "  41,\n",
       "  29,\n",
       "  42,\n",
       "  36,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  37,\n",
       "  -1,\n",
       "  5,\n",
       "  15,\n",
       "  11,\n",
       "  2,\n",
       "  47,\n",
       "  6,\n",
       "  27,\n",
       "  3,\n",
       "  50,\n",
       "  -1,\n",
       "  8,\n",
       "  24,\n",
       "  26,\n",
       "  19,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  2,\n",
       "  16,\n",
       "  10,\n",
       "  0,\n",
       "  4,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  2,\n",
       "  19,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  48,\n",
       "  8,\n",
       "  -1,\n",
       "  20,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  11,\n",
       "  44,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  50,\n",
       "  -1,\n",
       "  2,\n",
       "  47,\n",
       "  5,\n",
       "  7,\n",
       "  -1,\n",
       "  10,\n",
       "  13,\n",
       "  -1,\n",
       "  33,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  58,\n",
       "  -1,\n",
       "  42,\n",
       "  3,\n",
       "  5,\n",
       "  -1,\n",
       "  25,\n",
       "  -1,\n",
       "  13,\n",
       "  25,\n",
       "  14,\n",
       "  6,\n",
       "  -1,\n",
       "  45,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  46,\n",
       "  0,\n",
       "  39,\n",
       "  6,\n",
       "  19,\n",
       "  -1,\n",
       "  14,\n",
       "  43,\n",
       "  3,\n",
       "  -1,\n",
       "  0,\n",
       "  6,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  40,\n",
       "  33,\n",
       "  16,\n",
       "  37,\n",
       "  6,\n",
       "  24,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  13,\n",
       "  25,\n",
       "  -1,\n",
       "  -1,\n",
       "  19,\n",
       "  23,\n",
       "  22,\n",
       "  6,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  26,\n",
       "  46,\n",
       "  32,\n",
       "  0,\n",
       "  49,\n",
       "  14,\n",
       "  34,\n",
       "  -1,\n",
       "  1,\n",
       "  29,\n",
       "  30,\n",
       "  43,\n",
       "  28,\n",
       "  21,\n",
       "  10,\n",
       "  44,\n",
       "  24,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  22,\n",
       "  -1,\n",
       "  15,\n",
       "  0,\n",
       "  -1,\n",
       "  27,\n",
       "  -1,\n",
       "  -1,\n",
       "  22,\n",
       "  19,\n",
       "  -1,\n",
       "  27,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  18,\n",
       "  12,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  17,\n",
       "  33,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  21,\n",
       "  20,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  42,\n",
       "  1,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  3,\n",
       "  24,\n",
       "  -1,\n",
       "  19,\n",
       "  23,\n",
       "  0,\n",
       "  21,\n",
       "  32,\n",
       "  29,\n",
       "  19,\n",
       "  -1,\n",
       "  55,\n",
       "  9,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  1,\n",
       "  48,\n",
       "  7,\n",
       "  -1,\n",
       "  27,\n",
       "  35,\n",
       "  -1,\n",
       "  0,\n",
       "  23,\n",
       "  13,\n",
       "  47,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  9,\n",
       "  3,\n",
       "  47,\n",
       "  -1,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  6,\n",
       "  35,\n",
       "  -1,\n",
       "  10,\n",
       "  3,\n",
       "  4,\n",
       "  -1,\n",
       "  5,\n",
       "  48,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  26,\n",
       "  16,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  49,\n",
       "  46,\n",
       "  0,\n",
       "  0,\n",
       "  49,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  38,\n",
       "  -1,\n",
       "  13,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  20,\n",
       "  -1,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  -1,\n",
       "  9,\n",
       "  2,\n",
       "  0,\n",
       "  18,\n",
       "  11,\n",
       "  0,\n",
       "  -1,\n",
       "  34,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  24,\n",
       "  3,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  20,\n",
       "  32,\n",
       "  1,\n",
       "  -1,\n",
       "  10,\n",
       "  16,\n",
       "  -1,\n",
       "  13,\n",
       "  2,\n",
       "  10,\n",
       "  32,\n",
       "  4,\n",
       "  44,\n",
       "  5,\n",
       "  4,\n",
       "  33,\n",
       "  3,\n",
       "  -1,\n",
       "  5,\n",
       "  8,\n",
       "  57,\n",
       "  0,\n",
       "  21,\n",
       "  7,\n",
       "  22,\n",
       "  28,\n",
       "  0,\n",
       "  8,\n",
       "  14,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  9,\n",
       "  40,\n",
       "  39,\n",
       "  9,\n",
       "  6,\n",
       "  14,\n",
       "  39,\n",
       "  9,\n",
       "  21,\n",
       "  -1,\n",
       "  0,\n",
       "  2,\n",
       "  10,\n",
       "  3,\n",
       "  44,\n",
       "  7,\n",
       "  -1,\n",
       "  22,\n",
       "  -1,\n",
       "  23,\n",
       "  -1,\n",
       "  30,\n",
       "  16,\n",
       "  14,\n",
       "  12,\n",
       "  -1,\n",
       "  11,\n",
       "  29,\n",
       "  51,\n",
       "  2,\n",
       "  0,\n",
       "  14,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  51,\n",
       "  1,\n",
       "  13,\n",
       "  5,\n",
       "  14,\n",
       "  15,\n",
       "  41,\n",
       "  5,\n",
       "  46,\n",
       "  52,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  4,\n",
       "  16,\n",
       "  2,\n",
       "  2,\n",
       "  47,\n",
       "  1,\n",
       "  0,\n",
       "  58,\n",
       "  7,\n",
       "  6,\n",
       "  45,\n",
       "  52,\n",
       "  4,\n",
       "  2,\n",
       "  16,\n",
       "  42,\n",
       "  9,\n",
       "  -1,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  13,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  20,\n",
       "  27,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  57,\n",
       "  -1,\n",
       "  10,\n",
       "  5,\n",
       "  43,\n",
       "  7,\n",
       "  4,\n",
       "  18,\n",
       "  30,\n",
       "  18,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  57,\n",
       "  -1,\n",
       "  16,\n",
       "  7,\n",
       "  14,\n",
       "  40,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  29,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  16,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  -1,\n",
       "  10,\n",
       "  14,\n",
       "  47,\n",
       "  2,\n",
       "  0,\n",
       "  -1,\n",
       "  13,\n",
       "  0,\n",
       "  -1,\n",
       "  11,\n",
       "  2,\n",
       "  54,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  41,\n",
       "  17,\n",
       "  0,\n",
       "  11,\n",
       "  8,\n",
       "  40,\n",
       "  13,\n",
       "  25,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  23,\n",
       "  16,\n",
       "  40,\n",
       "  11,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  21,\n",
       "  28,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  11,\n",
       "  -1,\n",
       "  42,\n",
       "  6,\n",
       "  47,\n",
       "  31,\n",
       "  34,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  26,\n",
       "  4,\n",
       "  19,\n",
       "  37,\n",
       "  -1,\n",
       "  48,\n",
       "  12,\n",
       "  58,\n",
       "  11,\n",
       "  -1,\n",
       "  11,\n",
       "  53,\n",
       "  8,\n",
       "  45,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  3,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  45,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  16,\n",
       "  1,\n",
       "  15,\n",
       "  -1,\n",
       "  7,\n",
       "  44,\n",
       "  5,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  45,\n",
       "  15,\n",
       "  -1,\n",
       "  20,\n",
       "  8,\n",
       "  18,\n",
       "  -1,\n",
       "  44,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  31,\n",
       "  -1,\n",
       "  29,\n",
       "  7,\n",
       "  54,\n",
       "  -1,\n",
       "  37,\n",
       "  43,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  18,\n",
       "  47,\n",
       "  5,\n",
       "  3,\n",
       "  -1,\n",
       "  21,\n",
       "  2,\n",
       "  4,\n",
       "  49,\n",
       "  11,\n",
       "  41,\n",
       "  -1,\n",
       "  26,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  ...],\n",
       " array([0.        , 0.72252376, 1.        , ..., 0.86042012, 1.        ,\n",
       "        1.        ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1,\n",
       "  22,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  37,\n",
       "  23,\n",
       "  12,\n",
       "  16,\n",
       "  -1,\n",
       "  0,\n",
       "  52,\n",
       "  50,\n",
       "  0,\n",
       "  44,\n",
       "  11,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  19,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  -1,\n",
       "  16,\n",
       "  6,\n",
       "  4,\n",
       "  31,\n",
       "  34,\n",
       "  -1,\n",
       "  8,\n",
       "  20,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  43,\n",
       "  -1,\n",
       "  -1,\n",
       "  36,\n",
       "  3,\n",
       "  12,\n",
       "  -1,\n",
       "  12,\n",
       "  10,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  44,\n",
       "  3,\n",
       "  4,\n",
       "  -1,\n",
       "  31,\n",
       "  32,\n",
       "  -1,\n",
       "  8,\n",
       "  38,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  3,\n",
       "  1,\n",
       "  47,\n",
       "  21,\n",
       "  7,\n",
       "  5,\n",
       "  38,\n",
       "  24,\n",
       "  4,\n",
       "  29,\n",
       "  24,\n",
       "  21,\n",
       "  42,\n",
       "  58,\n",
       "  43,\n",
       "  15,\n",
       "  25,\n",
       "  10,\n",
       "  0,\n",
       "  15,\n",
       "  9,\n",
       "  0,\n",
       "  36,\n",
       "  11,\n",
       "  29,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  14,\n",
       "  9,\n",
       "  28,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  13,\n",
       "  -1,\n",
       "  47,\n",
       "  23,\n",
       "  2,\n",
       "  22,\n",
       "  37,\n",
       "  22,\n",
       "  -1,\n",
       "  0,\n",
       "  32,\n",
       "  12,\n",
       "  3,\n",
       "  41,\n",
       "  10,\n",
       "  -1,\n",
       "  20,\n",
       "  23,\n",
       "  -1,\n",
       "  38,\n",
       "  -1,\n",
       "  4,\n",
       "  2,\n",
       "  16,\n",
       "  28,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  40,\n",
       "  54,\n",
       "  4,\n",
       "  11,\n",
       "  18,\n",
       "  -1,\n",
       "  0,\n",
       "  36,\n",
       "  11,\n",
       "  15,\n",
       "  -1,\n",
       "  8,\n",
       "  40,\n",
       "  -1,\n",
       "  47,\n",
       "  40,\n",
       "  18,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  0,\n",
       "  50,\n",
       "  34,\n",
       "  9,\n",
       "  20,\n",
       "  9,\n",
       "  8,\n",
       "  51,\n",
       "  18,\n",
       "  -1,\n",
       "  39,\n",
       "  42,\n",
       "  29,\n",
       "  47,\n",
       "  3,\n",
       "  8,\n",
       "  -1,\n",
       "  26,\n",
       "  35,\n",
       "  -1,\n",
       "  56,\n",
       "  -1,\n",
       "  12,\n",
       "  35,\n",
       "  -1,\n",
       "  20,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  31,\n",
       "  42,\n",
       "  34,\n",
       "  21,\n",
       "  8,\n",
       "  4,\n",
       "  43,\n",
       "  -1,\n",
       "  0,\n",
       "  50,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  13,\n",
       "  1,\n",
       "  28,\n",
       "  30,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  6,\n",
       "  11,\n",
       "  34,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  36,\n",
       "  8,\n",
       "  51,\n",
       "  2,\n",
       "  13,\n",
       "  25,\n",
       "  16,\n",
       "  2,\n",
       "  0,\n",
       "  4,\n",
       "  39,\n",
       "  10,\n",
       "  8,\n",
       "  30,\n",
       "  46,\n",
       "  10,\n",
       "  7,\n",
       "  -1,\n",
       "  -1,\n",
       "  32,\n",
       "  43,\n",
       "  16,\n",
       "  11,\n",
       "  27,\n",
       "  22,\n",
       "  3,\n",
       "  -1,\n",
       "  14,\n",
       "  13,\n",
       "  8,\n",
       "  1,\n",
       "  -1,\n",
       "  39,\n",
       "  14,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  55,\n",
       "  2,\n",
       "  1,\n",
       "  24,\n",
       "  56,\n",
       "  -1,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  27,\n",
       "  6,\n",
       "  18,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  22,\n",
       "  36,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  14,\n",
       "  46,\n",
       "  -1,\n",
       "  10,\n",
       "  -1,\n",
       "  14,\n",
       "  2,\n",
       "  1,\n",
       "  6,\n",
       "  14,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  14,\n",
       "  28,\n",
       "  26,\n",
       "  41,\n",
       "  17,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  1,\n",
       "  -1,\n",
       "  15,\n",
       "  0,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  4,\n",
       "  -1,\n",
       "  12,\n",
       "  1,\n",
       "  6,\n",
       "  39,\n",
       "  30,\n",
       "  10,\n",
       "  18,\n",
       "  16,\n",
       "  42,\n",
       "  12,\n",
       "  8,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  52,\n",
       "  0,\n",
       "  12,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  -1,\n",
       "  2,\n",
       "  24,\n",
       "  13,\n",
       "  28,\n",
       "  1,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  35,\n",
       "  -1,\n",
       "  2,\n",
       "  8,\n",
       "  36,\n",
       "  -1,\n",
       "  0,\n",
       "  20,\n",
       "  -1,\n",
       "  3,\n",
       "  5,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  15,\n",
       "  26,\n",
       "  1,\n",
       "  33,\n",
       "  40,\n",
       "  3,\n",
       "  22,\n",
       "  0,\n",
       "  21,\n",
       "  -1,\n",
       "  -1,\n",
       "  13,\n",
       "  -1,\n",
       "  28,\n",
       "  29,\n",
       "  14,\n",
       "  35,\n",
       "  -1,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  3,\n",
       "  23,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  2,\n",
       "  -1,\n",
       "  24,\n",
       "  58,\n",
       "  8,\n",
       "  2,\n",
       "  -1,\n",
       "  2,\n",
       "  6,\n",
       "  17,\n",
       "  -1,\n",
       "  7,\n",
       "  15,\n",
       "  -1,\n",
       "  42,\n",
       "  4,\n",
       "  39,\n",
       "  41,\n",
       "  29,\n",
       "  42,\n",
       "  36,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  -1,\n",
       "  37,\n",
       "  -1,\n",
       "  5,\n",
       "  15,\n",
       "  11,\n",
       "  2,\n",
       "  47,\n",
       "  6,\n",
       "  27,\n",
       "  3,\n",
       "  50,\n",
       "  -1,\n",
       "  8,\n",
       "  24,\n",
       "  26,\n",
       "  19,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  2,\n",
       "  16,\n",
       "  10,\n",
       "  0,\n",
       "  4,\n",
       "  10,\n",
       "  12,\n",
       "  7,\n",
       "  2,\n",
       "  19,\n",
       "  12,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  48,\n",
       "  8,\n",
       "  -1,\n",
       "  20,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  11,\n",
       "  44,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  50,\n",
       "  -1,\n",
       "  2,\n",
       "  47,\n",
       "  5,\n",
       "  7,\n",
       "  -1,\n",
       "  10,\n",
       "  13,\n",
       "  -1,\n",
       "  33,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  58,\n",
       "  -1,\n",
       "  42,\n",
       "  3,\n",
       "  5,\n",
       "  -1,\n",
       "  25,\n",
       "  -1,\n",
       "  13,\n",
       "  25,\n",
       "  14,\n",
       "  6,\n",
       "  -1,\n",
       "  45,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  46,\n",
       "  0,\n",
       "  39,\n",
       "  6,\n",
       "  19,\n",
       "  -1,\n",
       "  14,\n",
       "  43,\n",
       "  3,\n",
       "  -1,\n",
       "  0,\n",
       "  6,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  40,\n",
       "  33,\n",
       "  16,\n",
       "  37,\n",
       "  6,\n",
       "  24,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  13,\n",
       "  25,\n",
       "  -1,\n",
       "  -1,\n",
       "  19,\n",
       "  23,\n",
       "  22,\n",
       "  6,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  -1,\n",
       "  26,\n",
       "  46,\n",
       "  32,\n",
       "  0,\n",
       "  49,\n",
       "  14,\n",
       "  34,\n",
       "  -1,\n",
       "  1,\n",
       "  29,\n",
       "  30,\n",
       "  43,\n",
       "  28,\n",
       "  21,\n",
       "  10,\n",
       "  44,\n",
       "  24,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  22,\n",
       "  -1,\n",
       "  15,\n",
       "  0,\n",
       "  -1,\n",
       "  27,\n",
       "  -1,\n",
       "  -1,\n",
       "  22,\n",
       "  19,\n",
       "  -1,\n",
       "  27,\n",
       "  4,\n",
       "  -1,\n",
       "  -1,\n",
       "  18,\n",
       "  12,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  17,\n",
       "  33,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  21,\n",
       "  20,\n",
       "  -1,\n",
       "  -1,\n",
       "  10,\n",
       "  42,\n",
       "  1,\n",
       "  -1,\n",
       "  21,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  3,\n",
       "  24,\n",
       "  -1,\n",
       "  19,\n",
       "  23,\n",
       "  0,\n",
       "  21,\n",
       "  32,\n",
       "  29,\n",
       "  19,\n",
       "  -1,\n",
       "  55,\n",
       "  9,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  1,\n",
       "  48,\n",
       "  7,\n",
       "  -1,\n",
       "  27,\n",
       "  35,\n",
       "  -1,\n",
       "  0,\n",
       "  23,\n",
       "  13,\n",
       "  47,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  9,\n",
       "  3,\n",
       "  47,\n",
       "  -1,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  6,\n",
       "  35,\n",
       "  -1,\n",
       "  10,\n",
       "  3,\n",
       "  4,\n",
       "  -1,\n",
       "  5,\n",
       "  48,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  26,\n",
       "  16,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  -1,\n",
       "  49,\n",
       "  46,\n",
       "  0,\n",
       "  0,\n",
       "  49,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  5,\n",
       "  38,\n",
       "  -1,\n",
       "  13,\n",
       "  0,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  20,\n",
       "  -1,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  -1,\n",
       "  9,\n",
       "  2,\n",
       "  0,\n",
       "  18,\n",
       "  11,\n",
       "  0,\n",
       "  -1,\n",
       "  34,\n",
       "  -1,\n",
       "  -1,\n",
       "  6,\n",
       "  24,\n",
       "  3,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  20,\n",
       "  32,\n",
       "  1,\n",
       "  -1,\n",
       "  10,\n",
       "  16,\n",
       "  -1,\n",
       "  13,\n",
       "  2,\n",
       "  10,\n",
       "  32,\n",
       "  4,\n",
       "  44,\n",
       "  5,\n",
       "  4,\n",
       "  33,\n",
       "  3,\n",
       "  -1,\n",
       "  5,\n",
       "  8,\n",
       "  57,\n",
       "  0,\n",
       "  21,\n",
       "  7,\n",
       "  22,\n",
       "  28,\n",
       "  0,\n",
       "  8,\n",
       "  14,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  9,\n",
       "  9,\n",
       "  40,\n",
       "  39,\n",
       "  9,\n",
       "  6,\n",
       "  14,\n",
       "  39,\n",
       "  9,\n",
       "  21,\n",
       "  -1,\n",
       "  0,\n",
       "  2,\n",
       "  10,\n",
       "  3,\n",
       "  44,\n",
       "  7,\n",
       "  -1,\n",
       "  22,\n",
       "  -1,\n",
       "  23,\n",
       "  -1,\n",
       "  30,\n",
       "  16,\n",
       "  14,\n",
       "  12,\n",
       "  -1,\n",
       "  11,\n",
       "  29,\n",
       "  51,\n",
       "  2,\n",
       "  0,\n",
       "  14,\n",
       "  -1,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  51,\n",
       "  1,\n",
       "  13,\n",
       "  5,\n",
       "  14,\n",
       "  15,\n",
       "  41,\n",
       "  5,\n",
       "  46,\n",
       "  52,\n",
       "  0,\n",
       "  -1,\n",
       "  7,\n",
       "  4,\n",
       "  16,\n",
       "  2,\n",
       "  2,\n",
       "  47,\n",
       "  1,\n",
       "  0,\n",
       "  58,\n",
       "  7,\n",
       "  6,\n",
       "  45,\n",
       "  52,\n",
       "  4,\n",
       "  2,\n",
       "  16,\n",
       "  42,\n",
       "  9,\n",
       "  -1,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  13,\n",
       "  -1,\n",
       "  4,\n",
       "  -1,\n",
       "  20,\n",
       "  27,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  57,\n",
       "  -1,\n",
       "  10,\n",
       "  5,\n",
       "  43,\n",
       "  7,\n",
       "  4,\n",
       "  18,\n",
       "  30,\n",
       "  18,\n",
       "  -1,\n",
       "  9,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  57,\n",
       "  -1,\n",
       "  16,\n",
       "  7,\n",
       "  14,\n",
       "  40,\n",
       "  -1,\n",
       "  -1,\n",
       "  4,\n",
       "  29,\n",
       "  5,\n",
       "  -1,\n",
       "  -1,\n",
       "  16,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  -1,\n",
       "  10,\n",
       "  14,\n",
       "  47,\n",
       "  2,\n",
       "  0,\n",
       "  -1,\n",
       "  13,\n",
       "  0,\n",
       "  -1,\n",
       "  11,\n",
       "  2,\n",
       "  54,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  2,\n",
       "  41,\n",
       "  17,\n",
       "  0,\n",
       "  11,\n",
       "  8,\n",
       "  40,\n",
       "  13,\n",
       "  25,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  23,\n",
       "  16,\n",
       "  40,\n",
       "  11,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  21,\n",
       "  28,\n",
       "  -1,\n",
       "  2,\n",
       "  -1,\n",
       "  8,\n",
       "  -1,\n",
       "  11,\n",
       "  -1,\n",
       "  42,\n",
       "  6,\n",
       "  47,\n",
       "  31,\n",
       "  34,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  26,\n",
       "  4,\n",
       "  19,\n",
       "  37,\n",
       "  -1,\n",
       "  48,\n",
       "  12,\n",
       "  58,\n",
       "  11,\n",
       "  -1,\n",
       "  11,\n",
       "  53,\n",
       "  8,\n",
       "  45,\n",
       "  12,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  3,\n",
       "  16,\n",
       "  -1,\n",
       "  -1,\n",
       "  11,\n",
       "  45,\n",
       "  2,\n",
       "  -1,\n",
       "  -1,\n",
       "  16,\n",
       "  1,\n",
       "  15,\n",
       "  -1,\n",
       "  7,\n",
       "  44,\n",
       "  5,\n",
       "  15,\n",
       "  -1,\n",
       "  -1,\n",
       "  8,\n",
       "  45,\n",
       "  15,\n",
       "  -1,\n",
       "  20,\n",
       "  8,\n",
       "  18,\n",
       "  -1,\n",
       "  44,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  5,\n",
       "  -1,\n",
       "  31,\n",
       "  -1,\n",
       "  29,\n",
       "  7,\n",
       "  54,\n",
       "  -1,\n",
       "  37,\n",
       "  43,\n",
       "  3,\n",
       "  -1,\n",
       "  -1,\n",
       "  18,\n",
       "  47,\n",
       "  5,\n",
       "  3,\n",
       "  -1,\n",
       "  21,\n",
       "  2,\n",
       "  4,\n",
       "  49,\n",
       "  11,\n",
       "  41,\n",
       "  -1,\n",
       "  26,\n",
       "  2,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  ...],\n",
       " array([0.        , 0.72252376, 1.        , ..., 0.86042012, 1.        ,\n",
       "        1.        ]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.7 matplotlib-3.9.4 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from config import config\n",
    "\n",
    "# 1. Load Wikipedia Data\n",
    "def load_wikipedia_data(sample_size=config.sample_size):\n",
    "    ds = tfds.load('wikipedia/20200301.en', split='train', shuffle_files=True)\n",
    "    texts = []\n",
    "    for example in ds.take(sample_size):  # Reduce for quick experimentation\n",
    "        texts.append(example['text'].numpy().decode('utf-8'))\n",
    "    return texts\n",
    "\n",
    "# 2. Preprocess Data (simple version)\n",
    "texts = load_wikipedia_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3. Create Document Embeddings using BERT\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/multi-qa-MiniLM-L6-dot-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mencode(texts, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 4. Cluster Topics with BERTopic\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:347\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/projects/fml/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Create Document Embeddings using BERT\n",
    "embedder = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-dot-v1', device='cuda')\n",
    "embeddings = embedder.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# 4. Cluster Topics with BERTopic\n",
    "topic_model = BERTopic(embedding_model=embedder, min_topic_size=15)\n",
    "topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# 5. Create Knowledge Graph Nodes (Topics)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_embeddings = np.array([topic_model.topic_embeddings_[topic] \n",
    "                           for topic in topic_info['Topic'] if topic != -1])\n",
    "\n",
    "# 6. Calculate Cosine Similarity for Edges\n",
    "similarity_matrix = cosine_similarity(topic_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Build Graph\n",
    "G = nx.Graph()\n",
    "threshold = 0.65  # Similarity threshold for edges\n",
    "\n",
    "# Add nodes with metadata\n",
    "for idx, row in topic_info.iterrows():\n",
    "    if row['Topic'] != -1:\n",
    "        G.add_node(row['Topic'],\n",
    "                   label=f\"Topic {row['Topic']}\",\n",
    "                   keywords=\", \".join([word[0] for word in topic_model.get_topic(row['Topic'])]),\n",
    "                   size=row['Count'])\n",
    "\n",
    "# Add edges based on similarity\n",
    "for i in range(len(topic_embeddings)):\n",
    "    for j in range(i+1, len(topic_embeddings)):\n",
    "        if similarity_matrix[i][j] > threshold:\n",
    "            G.add_edge(topic_info.iloc[i]['Topic'], \n",
    "                      topic_info.iloc[j]['Topic'],\n",
    "                      weight=similarity_matrix[i][j])\n",
    "\n",
    "# 8. Visualize Knowledge Graph\n",
    "plt.figure(figsize=(20, 15))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "\n",
    "node_sizes = [G.nodes[node]['size']*10 for node in G.nodes]\n",
    "edge_weights = [G.edges[edge]['weight']*2 for edge in G.edges]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.2)\n",
    "nx.draw_networkx_labels(G, pos, \n",
    "                        labels={node:G.nodes[node]['label'] for node in G.nodes},\n",
    "                        font_size=8)\n",
    "\n",
    "# Add keyword annotations\n",
    "for node in G.nodes:\n",
    "    plt.annotate(G.nodes[node]['keywords'], \n",
    "                 xy=pos[node], \n",
    "                 xytext=(10, -10),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=6,\n",
    "                 alpha=0.7)\n",
    "\n",
    "plt.title(\"Wikipedia Knowledge Graph (BERTopic + BERT Embeddings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.0.2)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
